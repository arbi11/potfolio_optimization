import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
import datetime
import yfinance as yf



import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from torch_geometric_temporal.nn.recurrent import A3TGCN2
from torch_geometric_temporal.signal import temporal_signal_split


from pathlib import Path
data_dir = Path(r'F:\cc_data')
model_dir = data_dir.absolute() / 'models'


# GPU support
DEVICE = torch.device('cpu') # cuda
shuffle=True
batch_size = 32








start = datetime.datetime(2020, 1, 1)
end_train = datetime.datetime(2024, 4, 1)
end_test = datetime.datetime.now()








import yfinance as yf

tickers = ["SPY", "IVV", "VOO", "VTI", "QQQ", "VEA", "VUG", "VTV", "IEFA", "AGG"]

df_train = yf.download(tickers, start=start, end=end_train)
df_test = yf.download(tickers, start=end_train, end=end_test)
df_train.head()


import joblib
from sklearn.preprocessing import MinMaxScaler, StandardScaler

scaler = StandardScaler()

scaled_train_data = scaler.fit_transform(df_train)
scaled_test_data = scaler.transform(df_test)

joblib.dump(scaler, model_dir / 'scaler.pkl')


df_train = pd.DataFrame(scaled_train_data, index=df_train.index, columns= df_train.columns)
df_test = pd.DataFrame(scaled_test_data, index=df_test.index, columns= df_test.columns)
df_train.shape, df_test.shape


df_train.head()











df_train.index.min(), df_train.index.max()


df_test.index.min(), df_test.index.max()


# df.columns


df_train.to_csv(data_dir / 'top10_etf_train.csv')
df_test.to_csv(data_dir / 'top10_etf_test.csv')


data_numpy_test = df_test.values
data_numpy_test.shape


data_numpy = df_train.values
data_numpy.shape


arrays = {}
arrays_test = {}
for level in df_train.columns.levels[1]:
    arrays[level] = df_train.xs(level, axis=1, level=1).to_numpy()
    arrays_test[level] = df_test.xs(level, axis=1, level=1).to_numpy()


arrays.keys()


arrays['AGG'].shape








num_timesteps_in = 15
num_timesteps_out = 15


indices = [(i, i + num_timesteps_in + num_timesteps_out) for i in range(data_numpy.shape[0] - num_timesteps_in - num_timesteps_out + 1)]
len(indices)


indices[:5]


indices[-5:]





features = []
targets = []
for i, j in indices:
    
    features_bucket_list = []
    target_bucket_list = []
    
    for etf in arrays.keys():
        features_bucket_list.append(arrays[etf][i : i+num_timesteps_in, :].T)
        target_bucket_list.append(arrays[etf][i+num_timesteps_in : j, 0].T)
    
    features_array_stack = np.stack(features_bucket_list, axis=0)
    target_array_stack = np.stack(target_bucket_list, axis=0)
    
    features.append(features_array_stack)
    targets.append(target_array_stack)

train_input = np.stack(features, axis=0)
train_target = np.stack(targets, axis=0)

train_input.shape, train_target.shape





indices = [(i, i + num_timesteps_in + num_timesteps_out) for i in range(data_numpy_test.shape[0] - num_timesteps_in - num_timesteps_out + 1)]
len(indices)


indices[:5]


indices[-5:]


features = []
targets = []
for i, j in indices:
    
    features_bucket_list = []
    target_bucket_list = []
    
    for etf in arrays_test.keys():
        features_bucket_list.append(arrays_test[etf][i : i+num_timesteps_in, :].T)
        target_bucket_list.append(arrays_test[etf][i+num_timesteps_in : j, 0].T)
    
    features_array_stack = np.stack(features_bucket_list, axis=0)
    target_array_stack = np.stack(target_bucket_list, axis=0)
    
    features.append(features_array_stack)
    targets.append(target_array_stack)

test_input = np.stack(features, axis=0)
test_target = np.stack(targets, axis=0)

test_input.shape, test_target.shape








len(targets)


train_input.shape, train_target.shape


features_array_stack.shape, target_array_stack.shape


len(features_bucket_list), len(target_bucket_list)


features_bucket_list[0].shape, target_bucket_list[0].shape








# train_input = np.array(train_dataset.features) # (27399, 207, 2, 12) --> (, 10, 6, 15)
# train_target = np.array(train_dataset.targets) # (27399, 207, 12) --> (, 10, 15)
train_x_tensor = torch.from_numpy(train_input).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)
train_target_tensor = torch.from_numpy(train_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)
train_dataset_new = torch.utils.data.TensorDataset(train_x_tensor, train_target_tensor)
train_loader = torch.utils.data.DataLoader(train_dataset_new, batch_size=batch_size, shuffle=shuffle,drop_last=True)


test_x_tensor = torch.from_numpy(test_input).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)
test_target_tensor = torch.from_numpy(test_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)
test_dataset_new = torch.utils.data.TensorDataset(test_x_tensor, test_target_tensor)
test_loader = torch.utils.data.DataLoader(test_dataset_new, batch_size=batch_size, shuffle=shuffle,drop_last=True)





train_input.shape





num_nodes = 10


edge_index = torch.tensor([(i, j) for i in range(num_nodes) for j in range(num_nodes)], dtype=torch.long).t().contiguous()
edge_index.shape


edge_attr = np.ones(edge_index.shape[1])
edge_attr.shape


from torch_geometric_temporal.signal import StaticGraphTemporalSignal

# Create a StaticGraphTemporalSignal object
dataset = StaticGraphTemporalSignal(edge_index=edge_index, edge_weight=None, features=[train_input], targets=[train_target])
dataset_test = StaticGraphTemporalSignal(edge_index=edge_index, edge_weight=None, features=[test_input], targets=[test_target])

# dataset = StaticGraphTemporalSignal(edge_index, edge_attr, train_input, train_target)
# dataset_test = StaticGraphTemporalSignal(edge_index, edge_attr, test_input, test_target)








from torch_geometric_temporal.nn.recurrent import A3TGCN2

class TemporalGNN(torch.nn.Module):
    def __init__(self, node_features, periods, batch_size):
        super(TemporalGNN, self).__init__()
        # Attention Temporal Graph Convolutional Cell
        self.tgnn = A3TGCN2(in_channels = node_features,  
                            out_channels = 32, 
                            periods = periods,
                            batch_size = batch_size) # node_features=2, periods=12
        # Equals single-shot prediction
        self.linear = torch.nn.Linear(32, periods)

    def forward(self, x, edge_index):
        """
        x = Node features for T time steps
        edge_index = Graph edge indices
        """
        h = self.tgnn(x, edge_index) # x [b, 207, 2, 12]  returns h [b, 207, 12]
        h = F.relu(h) 
        h = self.linear(h)
        return h

TemporalGNN(node_features=6, periods=15, batch_size=20)


# Create model and optimizers
# model = TemporalGNN(node_features=6, periods=15, batch_size=batch_size).to(DEVICE)
model = TemporalGNN(node_features = train_input.shape[2], 
                    periods = train_input.shape[-1], 
                    batch_size=batch_size).to(DEVICE)

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_fn = torch.nn.MSELoss()

print('Net\'s state_dict:')
total_param = 0
for param_tensor in model.state_dict():
    print(param_tensor, '\t', model.state_dict()[param_tensor].size())
    total_param += np.prod(model.state_dict()[param_tensor].size())
print('Net\'s total params:', total_param)
#--------------------------------------------------
print('Optimizer\'s state_dict:')
for var_name in optimizer.state_dict():
    print(var_name, '\t', optimizer.state_dict()[var_name])


def evaluate_model(model, val_loader):
    loss = 0
    step = 0
    val_mape, val_mae = [], []
    
    model.eval()
    with torch.no_grad():
        for encoder_inputs, labels in val_loader:
            
            y_hat = model(encoder_inputs, static_edge_index)
            loss = loss + torch.mean(torch.abs(y_hat-labels)) 
            val_mape.append(mean_absolute_percentage_error(labels, y_hat))
            val_mae.append(mean_absolute_error(labels, y_hat))
            step += 1
        loss = loss / (step + 1)
    
    # print("Val MAE: {:.4f}".format(loss.item()))
    return loss, val_mape, val_mae





for snapshot in dataset:
    print(snapshot.x.shape, snapshot.edge_index.shape)
    break


for snapshot in dataset:
    static_edge_index = snapshot.edge_index.to(DEVICE)
    break;


def mean_absolute_error(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def mean_absolute_percentage_error(y_true, y_pred):
    return torch.mean(torch.abs((y_true - y_pred) / y_true)) * 100





model.train()

val_losses, train_losses = [], []
best_model = None
best_va_loss = 1E6


for epoch in range(501):
    step = 0
    
    loss_list = []
    total_mape = []
    total_mae = []
    
    for encoder_inputs, labels in train_loader:
        y_hat = model(encoder_inputs, static_edge_index)         # Get model predictions
        loss = loss_fn(y_hat, labels) # Mean squared error #loss = torch.mean((y_hat-labels)**2)  sqrt to change it to rmse
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        step= step+ 1
        
        loss_list.append(loss.item())
        total_mape.append(mean_absolute_percentage_error(labels, y_hat))
        total_mae.append(mean_absolute_error(labels, y_hat))
        
        if step % 100 == 0 :
            print(sum(loss_list)/len(loss_list))
    
    va_loss, val_mape, val_mae = evaluate_model(model, test_loader)
    val_losses.append(va_loss.item())
    train_losses.append(sum(loss_list)/len(loss_list))
    
    if epoch % 10 == 0 :
        print(f"Epoch {epoch} train RMSE: {sum(loss_list)/len(loss_list):.4E}, \
                MAE: {sum(total_mae)/len(total_mae):.4E}, \
                MAPE: {sum(total_mape)/len(total_mape):.2f}%"
             )

        print(f"\t \t Val RMSE: {sum(val_losses)/len(val_losses):.4E}, \
                MAE: {sum(val_mae)/len(val_mae):.4E}, \
                MAPE: {sum(val_mape)/len(val_mape):.2f}%"
             )

    if va_loss < best_va_loss:
        best_va_loss = va_loss
        best_model_state = model.state_dict()
        torch.save(best_model_state, model_dir.absolute() / 'best_A3TGCN_model.pth')
        print("\t \t Best model saved with validation loss: {:.4f}".format(va_loss.item()))

    





import matplotlib.pyplot as plt
plt.figure(figsize=(10,7))
plt.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss')
plt.plot(range(1, len(val_losses)+1), val_losses, label='Val Loss')
plt.legend(loc="upper left")
plt.show()





model.load_state_dict(torch.load(model_dir.absolute() / 'best_A3TGCN_model.pth'))






model.eval()
step = 0
# Store for analysis
total_loss = []
total_mape = []
total_mae = []

for encoder_inputs, labels in test_loader:
    # Get model predictions
    y_hat = model(encoder_inputs, static_edge_index)
    # Mean squared error
    loss = loss_fn(y_hat, labels)
    total_loss.append(loss.item())
    total_mape.append(mean_absolute_percentage_error(labels, y_hat))
    total_mae.append(mean_absolute_error(labels, y_hat))
    # Store for analysis below
    #test_labels.append(labels)
    #predictions.append(y_hat)

print("Test MSE: {:.4f}".format(sum(total_loss)/len(total_loss)))
print("Test MAE: {:.4f}".format(sum(total_mae)/len(total_mae)))
print("Test MAPE: {:.4f}%".format(sum(total_mape)/len(total_mape)))




labels.shape, y_hat.shape





sensor = 2
timestep = 0
preds = np.asarray([pred[sensor][timestep].detach().cpu().numpy() for pred in y_hat])
labs  = np.asarray([label[sensor][timestep].cpu().numpy() for label in labels])
print("Data points:,", preds.shape)


plt.style.use('ggplot')

plt.figure(figsize=(20,5))
sns.lineplot(data=preds, label="pred")
sns.lineplot(data=labs, label="true")











model.eval()
step = 0
# Store for analysis
total_loss = []
total_mape = []
total_mae = []

for encoder_inputs, labels in train_loader:
    # Get model predictions
    y_hat = model(encoder_inputs, static_edge_index)
    # Mean squared error
    loss = loss_fn(y_hat, labels)
    total_loss.append(loss.item())
    total_mape.append(mean_absolute_percentage_error(labels, y_hat))
    total_mae.append(mean_absolute_error(labels, y_hat))
    # Store for analysis below
    #test_labels.append(labels)
    #predictions.append(y_hat)

print("Test MSE: {:.4f}".format(sum(total_loss)/len(total_loss)))
print("Test MAE: {:.4f}".format(sum(total_mae)/len(total_mae)))
print("Test MAPE: {:.4f}%".format(sum(total_mape)/len(total_mape)))




labels.shape, y_hat.shape





sensor = 2
timestep = 0
preds = np.asarray([pred[sensor][timestep].detach().cpu().numpy() for pred in y_hat])
labs  = np.asarray([label[sensor][timestep].cpu().numpy() for label in labels])
print("Data points:,", preds.shape)


plt.style.use('ggplot')

plt.figure(figsize=(20,5))
sns.lineplot(data=preds, label="pred")
sns.lineplot(data=labs, label="true")









