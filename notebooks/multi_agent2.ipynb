{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "009a266f-a67f-4e9f-ad24-a7bd01f49339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from crewai import Agent, Task, Process, Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f97ba70c-8589-48cd-b5ee-f54b7a443d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akhan147\\Anaconda3\\envs\\trader\\lib\\site-packages\\langchain_community\\tools\\ddg_search\\__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchRun\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c08ba035-4445-4e2d-9671-bd11c31577dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = DuckDuckGoSearchRun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b587c593-75ca-41e1-bdce-616b3014a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "notebook_dir = Path('.')\n",
    "file_path = notebook_dir.absolute().parent / 'data' / 'secrets.txt'\n",
    "\n",
    "secret_dict = {}\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if '=' in line:\n",
    "            key, value = line.split('=', 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip().strip('\"')\n",
    "            secret_dict[key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05429499-3f3d-42be-99f3-b457998cdfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SERPER_API_KEY\"] = secret_dict[\"SERPER_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5eb589-ebc8-43fa-81c8-b978165321c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Initialize your local Ollama LLM model\n",
    "ollama_llm = Ollama(model=\"llama3.1:latest\")\n",
    "\n",
    "# Initialize the Google Serper API Wrapper\n",
    "google_serper = GoogleSerperAPIWrapper(api_key=secret_dict[\"SERPER_KEY\"])\n",
    "\n",
    "# Wrap the Google Serper API Wrapper in a RunnableLambda\n",
    "google_serper_runnable = RunnableLambda(lambda x: google_serper.run(x))\n",
    "\n",
    "# Define a prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"{query}\"\n",
    ")\n",
    "\n",
    "# Create a RunnableSequence using the pipe operator\n",
    "sequence = prompt_template | ollama_llm | google_serper_runnable\n",
    "\n",
    "# Example query\n",
    "query = \"latest AI research news\"\n",
    "\n",
    "# Run the sequence\n",
    "result = sequence.invoke(query)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4cb4c-65ec-4a8c-9ae6-6cddd47d7181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb727617-9d39-4968-97d1-bc169014dc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd7671e-e606-43f5-b6f2-5f68535b860f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28ffe1-4ac4-4bc0-853e-84e073ac0625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1509dd9c-4e0c-497e-ac3b-e6db53f8ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "# Initialize your local Ollama LLM model\n",
    "# ollama_llm = Ollama(model_path=\"path/to/your/local/ollama/model\")\n",
    "ollama_llm = Ollama(model=\"llama3.1:latest\")\n",
    "\n",
    "\n",
    "# Initialize the Google Serper API Wrapper\n",
    "google_serper = GoogleSerperAPIWrapper(api_key=secret_dict[\"SERPER_KEY\"])\n",
    "\n",
    "# Wrap the Google Serper API Wrapper in a RunnableLambda\n",
    "google_serper_runnable = RunnableLambda(lambda x: google_serper.run(x))\n",
    "\n",
    "# Define a prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"Search for: {query}\"\n",
    ")\n",
    "\n",
    "# Create a RunnableSequence using the pipe operator\n",
    "sequence = prompt_template | ollama_llm | google_serper_runnable\n",
    "\n",
    "# Example query\n",
    "query = \"latest news on AI research\"\n",
    "\n",
    "# Run the sequence\n",
    "result = sequence.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94b328-f8f9-441d-8fba-cfea69095590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d583580-0a8e-46b9-ae13-e087a457ba69",
   "metadata": {},
   "source": [
    "Approach 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f68bd9d-7218-4597-af1c-3e39eb23df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "\n",
    "# Initialize SerpAPI tool with your API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"Your Key\"\n",
    "os.environ[\"SERPER_API_KEY\"] = secret_dict[\"SERPER_KEY\"]\n",
    "\n",
    "search = GoogleSerperAPIWrapper()\n",
    "\n",
    "# Create tool to be used by agent\n",
    "search_tool = Tool(\n",
    "  name=\"Intermediate Answer\",\n",
    "  func=search.run,\n",
    "  description=\"useful for when you need to ask with search\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7def56-e656-4f0b-996c-c056ea908164",
   "metadata": {},
   "outputs": [],
   "source": [
    "explorer = Agent(\n",
    "    role='Senior Researcher',\n",
    "    goal='Find and explore the most exciting projects and companies in AI and machine learning in 2024',\n",
    "    backstory=\"\"\"You are and Expert strategist that knows how to spot\n",
    "    emerging trends and companies in AI, tech and machine learning. \n",
    "    You're great at finding interesting, exciting projects in Open \n",
    "    Source/Indie Hacker space. You look for exciting, rising\n",
    "    new github ai projects that get a lot of attention.\n",
    "    \"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    tools=[search_tool]\n",
    ")\n",
    "writer = Agent(\n",
    "    role='Senior Technical Writer',\n",
    "    goal='Write engaging and interesting blog post about latest AI projects using simple, layman vocabulary',\n",
    "    backstory=\"\"\"You are an Expert Writer on technical innovation, especially \n",
    "    in the field of AI and machine learning. You know how to write in \n",
    "    engaging, interesting but simple, straightforward and concise. You know \n",
    "    how to present complicated technical terms to general audience in a \n",
    "    fun way by using layman words.\"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=True\n",
    ")\n",
    "critic = Agent(\n",
    "    role='Expert Writing Critic',\n",
    "    goal='Provide feedback and criticize blog post drafts. Make sure that the tone and writing style is compelling, simple and concise',\n",
    "    backstory=\"\"\"You are an Expert at providing feedback to the technical\n",
    "    writers. You can tell when a blog text isn't concise,\n",
    "    simple or engaging enough. You know how to provide helpful feedback that \n",
    "    can improve any text. You know how to make sure that text \n",
    "    stays technical and insightful by using layman terms.\n",
    "    \"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e78265-561d-44f4-acdf-51720156dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_report = Task(\n",
    "    description=\"\"\"Use and summarize scraped data from subreddit LocalLLama to\n",
    "    make a detailed report on the latest rising projects in AI. Your final \n",
    "    answer MUST be a full analysis report, text only, ignore any code or \n",
    "    anything that isn't text. The report has to have bullet points and with\n",
    "    5-10 exciting new AI projects and tools. Write names of every tool and \n",
    "    project. Each bullet point MUST contain 3 sentences that refer to one \n",
    "    specific ai company, product, model or anything you found on subreddit \n",
    "    LocalLLama. Use ONLY scraped data from LocalLLama to generate the report. \n",
    "    \"\"\",\n",
    "    agent=explorer,\n",
    ")\n",
    "\n",
    "task_blog = Task(\n",
    "    description=\"\"\"Write a blog article with text only and with a short but \n",
    "    impactful headline and at least 10 paragraphs. Blog should summarize \n",
    "    the report on latest ai tools found on localLLama subreddit. Style and \n",
    "    tone should be compelling and concise, fun, technical but also use \n",
    "    layman words for the general public. Name specific new, exciting projects,\n",
    "    apps and companies in AI world. Don't write \"**Paragraph [number of the \n",
    "    paragraph]:**\", instead start the new paragraph in a new line. Write names\n",
    "    of projects and tools in BOLD.\n",
    "    ALWAYS include links to projects/tools/research papers.\n",
    "    \"\"\",\n",
    "    agent=writer,\n",
    ")\n",
    "\n",
    "task_critique = Task(\n",
    "    description=\"\"\"Identify parts of the blog that aren't written concise \n",
    "    enough and rewrite and change them. Make sure that the blog has engaging \n",
    "    headline with 30 characters max, and that there are at least 10 paragraphs.\n",
    "    Blog needs to be rewritten in such a way that it contains specific \n",
    "    names of models/companies/projects but also explanation of WHY a reader\n",
    "    should be interested to research more. Always include links to each paper/\n",
    "    project/company\n",
    "    \"\"\",\n",
    "    agent=critic,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec4abc6-d896-48f2-b426-320b001a95cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate crew of agents\n",
    "crew = Crew(\n",
    "    agents=[explorer, writer, critic],\n",
    "    tasks=[task_report, task_blog, task_critique],\n",
    "    verbose=2,\n",
    "    # Sequential process will have tasks executed one after the other and the outcome of the previous one is passed as extra content into this next.\n",
    "    process=Process.sequential \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0890c00d-f4c6-44e0-8450-779978d40983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your crew to work!\n",
    "result = crew.kickoff()\n",
    "\n",
    "print(\"######################\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83d4538-1522-42d7-be0e-642ed3c60528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c891f82-23be-4fd7-8068-25d492656819",
   "metadata": {},
   "source": [
    "# Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8211ff6-74a6-4eae-b357-3720f1ff3d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b6ec53d-5297-442e-a3c5-bdffa6c7b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "UG = \"Scraper 3.7 u/arbi11\"\n",
    "reddit = praw.Reddit(\n",
    "            client_id=secret_dict[\"REDDIT_CLIENT_ID\"],\n",
    "            client_secret=secret_dict[\"REDDIT_CLIENT_SECRET\"],\n",
    "            user_agent=UG,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49ed6a32-6211-4863-9bdf-a476a94a3d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create empty lists to store data\n",
    "comments = []\n",
    "comment_authors = []\n",
    "comment_timestamps = []\n",
    "\n",
    "# Iterate through the hot submissions in the specified subreddit\n",
    "for submission in reddit.subreddit(\"News\").hot(limit=None):  # Adjust the limit as needed\n",
    " # Iterate through submission comments\n",
    "    for comment in submission.comments.list():\n",
    "        # Check if the comment is an instance of MoreComments\n",
    "        if isinstance(comment, praw.models.MoreComments):\n",
    "            continue  # Skip MoreComments objects\n",
    "\n",
    "        # Append comment information\n",
    "        comments.append(comment.body)\n",
    "        comment_authors.append(comment.author.name if comment.author else \"Unknown\")\n",
    "\n",
    "        # Convert timestamp to human-readable date and time\n",
    "        timestamp = datetime.utcfromtimestamp(comment.created_utc)\n",
    "        comment_timestamps.append(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "341bc970-b8b2-42ca-bd5e-d971c249a24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Comment      Comment Author  \\\n",
      "0  > “If you’re a registered voter in PA, GA, NV,...       Panic_Azimuth   \n",
      "1  If a democrat billionaire like Soros started t...         CrimsonR4ge   \n",
      "2   But don’t hand water to people standing in line.  OSI_Hunter_Gathers   \n",
      "3  I love that Cards Against Humanity is back in ...  tronaldrumptochina   \n",
      "4  Donald Trump is a human toilet.\\n\\nNo need to ...               rnilf   \n",
      "\n",
      "    Comment Timestamp  \n",
      "0 2024-10-11 20:44:30  \n",
      "1 2024-10-11 20:55:59  \n",
      "2 2024-10-11 22:12:04  \n",
      "3 2024-10-11 22:07:31  \n",
      "4 2024-10-11 20:39:45  \n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Comment': comments,\n",
    "    'Comment Author': comment_authors,\n",
    "    'Comment Timestamp': comment_timestamps\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('reddit_comments_dataset.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "072a6f45-e816-4b64-9a96-34c5e59eb94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create empty lists to store data\n",
    "titles = []\n",
    "submission_ids = []\n",
    "authors = []\n",
    "scores = []\n",
    "upvote_ratios = []\n",
    "num_comments = []\n",
    "urls = []\n",
    "\n",
    "for submission in reddit.subreddit('News').hot(limit=None):\n",
    "\n",
    "    # Append data to lists\n",
    "    titles.append(submission.title)\n",
    "    submission_ids.append(submission.id)\n",
    "    authors.append(submission.author)\n",
    "    scores.append(submission.score)\n",
    "    upvote_ratios.append(submission.upvote_ratio)\n",
    "    num_comments.append(submission.num_comments)\n",
    "    urls.append(submission.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d0a6fc6-6b46-4e68-8bd4-dba30a19644e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Title Submission ID  \\\n",
      "0     Cards Against Humanity offers payouts to new ...       1g1ixeg   \n",
      "1    Dismembered remains found in freezer identifie...       1g1r3eh   \n",
      "2    Montana businessman gets 2 years in prison for...       1g1odva   \n",
      "3    US meteorologists face death threats as hurric...       1g1d91d   \n",
      "4    15-year-old girl sentenced to 3 years for kill...       1g1mbrz   \n",
      "..                                                 ...           ...   \n",
      "189  Helene floodwaters trapped Tennessee plastics ...       1ftc7i6   \n",
      "190    Baseball legend Pete Rose dies at the age of 83       1ft9rnu   \n",
      "191      State judge strikes down Georgia abortion ban       1ft65bh   \n",
      "192  'Hamas leader' in Lebanon killed by Israel was...       1ft5ym9   \n",
      "193  Israel says 'localised, targeted ground raids'...       1ft9sx2   \n",
      "\n",
      "                   Author  Score  Upvote Ratio  Number of Comments  \\\n",
      "0              raresaturn  18805          0.92                 436   \n",
      "1          Right-Might664   2977          0.98                 204   \n",
      "2        WhileFalseRepeat   4292          0.97                 133   \n",
      "3             utrecht1976  30892          0.93                3024   \n",
      "4    Master_Jackfruit3591   2369          0.96                 241   \n",
      "..                    ...    ...           ...                 ...   \n",
      "189              vikingzx   7443          0.98                 331   \n",
      "190         VeryPerry1120  11488          0.92                1005   \n",
      "191           spherocytes  11558          0.98                 161   \n",
      "192             sadandshy   7987          0.88                 673   \n",
      "193             FruitOnyx   3985          0.90                1440   \n",
      "\n",
      "                                                   URL  \n",
      "0    https://www.nbcnews.com/tech/internet/cards-hu...  \n",
      "1    https://www.wjhg.com/2024/10/11/dismembered-re...  \n",
      "2    https://apnews.com/article/montana-businessman...  \n",
      "3    https://www.theguardian.com/us-news/2024/oct/1...  \n",
      "4    https://www.fox5dc.com/news/15-year-old-girl-s...  \n",
      "..                                                 ...  \n",
      "189  https://www.nbcnews.com/news/latino/hurricane-...  \n",
      "190  https://abcnews.go.com/US/Culture/baseball-leg...  \n",
      "191  https://www.nbcnews.com/politics/politics-news...  \n",
      "192  https://news.sky.com/story/hamas-leader-in-leb...  \n",
      "193       https://www.bbc.co.uk/news/live/cg4qx62kkxxt  \n",
      "\n",
      "[194 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame using the lists\n",
    "df = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Submission ID': submission_ids,\n",
    "    'Author': authors,\n",
    "    'Score': scores,\n",
    "    'Upvote Ratio': upvote_ratios,\n",
    "    'Number of Comments': num_comments,\n",
    "    'URL': urls\n",
    "})\n",
    "\n",
    "# Print or manipulate the DataFrame as needed\n",
    "print(df)\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('reddit_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f581c5de-b7b6-4bf6-a2cb-47196d3f0366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from crewai import Agent, Task, Process, Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c3a9275-4e56-485a-a997-e58688c16874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "ollama_llm = Ollama(model=\"mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc5613cd-e269-455b-a065-a9a2c7f3e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "notebook_dir = Path('.')\n",
    "file_path = notebook_dir.absolute().parent / 'data' / 'secrets.txt'\n",
    "\n",
    "secret_dict = {}\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if '=' in line:\n",
    "            key, value = line.split('=', 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip().strip('\"')\n",
    "            secret_dict[key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c8d766f-d28e-4625-884c-bf4ccf9f862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "class BrowserTool():\n",
    "    @tool(\"Scrape reddit content\")\n",
    "    def scrape_reddit(max_comments_per_post=5):\n",
    "        \"\"\"Useful to scrape a reddit content\"\"\"\n",
    "        reddit = praw.Reddit(\n",
    "            client_id=secret_dict[\"REDDIT_CLIENT_ID\"],\n",
    "            client_secret=secret_dict[\"REDDIT_CLIENT_SECRET\"],\n",
    "            user_agent=UG,\n",
    "        )\n",
    "        subreddit = reddit.subreddit(\"LocalLLaMA\")\n",
    "        scraped_data = []\n",
    "\n",
    "        for post in subreddit.hot(limit=10):\n",
    "            post_data = {\"title\": post.title, \"url\": post.url, \"comments\": []}\n",
    "\n",
    "            try:\n",
    "                post.comments.replace_more(limit=0)  # Load top-level comments only\n",
    "                comments = post.comments.list()\n",
    "                if max_comments_per_post is not None:\n",
    "                    comments = comments[:5]\n",
    "\n",
    "                for comment in comments:\n",
    "                    post_data[\"comments\"].append(comment.body)\n",
    "\n",
    "                scraped_data.append(post_data)\n",
    "\n",
    "            except praw.exceptions.APIException as e:\n",
    "                print(f\"API Exception: {e}\")\n",
    "                time.sleep(60)  # Sleep for 1 minute before retrying\n",
    "\n",
    "        return scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b38d8ff-68b3-43ad-904c-964bdd31c683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 06:31:29,249 - 43732 - llm.py-llm:178 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explorer = Agent(\n",
    "    role=\"Senior Researcher\",\n",
    "    goal=\"Find and explore the most exciting projects and companies in AI and machine learning in 2024\",\n",
    "    backstory=\"\"\"You are and Expert strategist that knows how to spot emerging trends and companies in AI, tech and machine learning. \n",
    "    You're great at finding interesting, exciting projects in Open Source/Indie Hacker space. You look for exciting, rising\n",
    "    new github ai projects that get a lot of attention.\n",
    "    \"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    tools=[BrowserTool().scrape_reddit],\n",
    "    llm=ollama_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b8de1cf-b63a-4115-8a52-88a4a2e03ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 06:31:34,487 - 43732 - llm.py-llm:178 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "2024-10-12 06:31:34,490 - 43732 - llm.py-llm:178 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "writer = Agent(\n",
    "    role='Senior Technical Writer',\n",
    "    goal='Write engaging and interesting blog post about latest AI projects using simple, layman vocabulary',\n",
    "    backstory=\"\"\"You are an Expert Writer on technical innovation, especially \n",
    "    in the field of AI and machine learning. You know how to write in \n",
    "    engaging, interesting but simple, straightforward and concise. You know \n",
    "    how to present complicated technical terms to general audience in a \n",
    "    fun way by using layman words.\"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=True,\n",
    "    llm=ollama_llm\n",
    ")\n",
    "critic = Agent(\n",
    "    role='Expert Writing Critic',\n",
    "    goal='Provide feedback and criticize blog post drafts. Make sure that the tone and writing style is compelling, simple and concise',\n",
    "    backstory=\"\"\"You are an Expert at providing feedback to the technical\n",
    "    writers. You can tell when a blog text isn't concise,\n",
    "    simple or engaging enough. You know how to provide helpful feedback that \n",
    "    can improve any text. You know how to make sure that text \n",
    "    stays technical and insightful by using layman terms.\n",
    "    \"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=True,\n",
    "    llm=ollama_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90abf676-20e0-4a71-9eee-b19c27862f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_report = Task(\n",
    "    description=\"\"\"Use and summarize scraped data from subreddit LocalLLama to\n",
    "    make a detailed report on the latest rising projects in AI. Your final \n",
    "    answer MUST be a full analysis report, text only, ignore any code or \n",
    "    anything that isn't text. The report has to have bullet points and with\n",
    "    5-10 exciting new AI projects and tools. Write names of every tool and \n",
    "    project. Each bullet point MUST contain 3 sentences that refer to one \n",
    "    specific ai company, product, model or anything you found on subreddit \n",
    "    LocalLLama. Use ONLY scraped data from LocalLLama to generate the report. \n",
    "    \"\"\",\n",
    "    agent=explorer,\n",
    ")\n",
    "\n",
    "task_blog = Task(\n",
    "    description=\"\"\"Write a blog article with text only and with a short but \n",
    "    impactful headline and at least 10 paragraphs. Blog should summarize \n",
    "    the report on latest ai tools found on localLLama subreddit. Style and \n",
    "    tone should be compelling and concise, fun, technical but also use \n",
    "    layman words for the general public. Name specific new, exciting projects,\n",
    "    apps and companies in AI world. Don't write \"**Paragraph [number of the \n",
    "    paragraph]:**\", instead start the new paragraph in a new line. Write names\n",
    "    of projects and tools in BOLD.\n",
    "    ALWAYS include links to projects/tools/research papers.\n",
    "    \"\"\",\n",
    "    agent=writer,\n",
    ")\n",
    "\n",
    "task_critique = Task(\n",
    "    description=\"\"\"Identify parts of the blog that aren't written concise \n",
    "    enough and rewrite and change them. Make sure that the blog has engaging \n",
    "    headline with 30 characters max, and that there are at least 10 paragraphs.\n",
    "    Blog needs to be rewritten in such a way that it contains specific \n",
    "    names of models/companies/projects but also explanation of WHY a reader\n",
    "    should be interested to research more. Always include links to each paper/\n",
    "    project/company\n",
    "    \"\"\",\n",
    "    agent=critic,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ce041-7cd2-436f-8a91-a5a0ead7a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate crew of agents\n",
    "crew = Crew(\n",
    "    agents=[explorer, writer, critic],\n",
    "    tasks=[task_report, task_blog, task_critique],\n",
    "    verbose=2,\n",
    "    process=Process.sequential,  # Sequential process will have tasks executed one after the other and the outcome of the previous one is passed as extra content into this next.\n",
    ")\n",
    "\n",
    "# Get your crew to work!\n",
    "result = crew.kickoff()\n",
    "\n",
    "print(\"######################\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce069a-eaa2-48ce-86cb-476ed2ab86d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
