{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the notebook, make sure the following dependencies are installed:\n",
    "\n",
    "1. LlamaIndex\n",
    "    * Open source framework to connect your LLMs with external data sources.\n",
    "        * pdfs, websites, APIs.\n",
    "        * supports popular LLMs & vector databases.\n",
    "    * simplifies indexing & querying data.\n",
    "\n",
    "2. Qdrant\n",
    "    * open source vector database optimized for querying high dimensional vectors\n",
    "    * Use docker to self host a Qdrant vector database\n",
    "    * provides fast similarity and filtering search\n",
    "    * Install Docker Engine (if not already)\n",
    "    * Start the docker container for qdrant\n",
    "    * `docker run -p 6333:6333 -p 6334:6334 \\\n",
    "    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n",
    "    qdrant/qdrant`\n",
    "        * `docker run`: Initiates a new container.\n",
    "        * `-p 6333:6333 -p 6334:6334`: Maps the host machine's ports 6333 and 6334 to the same ports on the container.\n",
    "        * `-v $(pwd)/qdrant_storage:/qdrant/storage:z`:\n",
    "            * `$(pwd)/qdrant_storage` is a directory on the host (the current working directory with a subdirectory named qdrant_storage).\n",
    "            * `/qdrant/storage` is the corresponding directory inside the container where Qdrant will store its data.\n",
    "            * The `:z` suffix sets the SELinux context for the volume (on SELinux-enabled systems), allowing the container to access the mounted volume securely.\n",
    "        * `qdrant/qdrant`: Specifies the Docker image to use, which is `qdrant/qdrant` from Docker Hub set up by the Qdrant team.\n",
    "\n",
    "3. Ollama\n",
    "    * provides a platform to run LLMs locally, giivng control over the data and model usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To handle asynchronous operations within a Jupyter Notebook environment, \n",
    "# which will allow us to make asynchronous calls smoothly in our RAG \n",
    "\n",
    "# nest_asyncio is a library that allows you to use asyncio in environments \n",
    "# that don't natively support it, like Jupyter Notebooks.\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "# This line modifies the event loop policy to allow nested use of asyncio.run(). Essentially, \n",
    "# it makes sure that the event loop can handle multiple asynchronous calls without running into issues.\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Qdrant vector database\n",
    "\n",
    " set up a connection to Qdrant (our local vector database), where we will store and retrieve vector embeddings in this RAG\n",
    "\n",
    "* Collections in Qdrant are like tables in databases, \n",
    "    * where each collection can hold a set of vectors. \n",
    "* Here, \"chat_with_docs\" is intended to store document embeddings \n",
    "    * to support query-based information retrieval.\n",
    "* client = qdrant_client.QdrantClient(...) initializes a QdrantClient instance, \n",
    "    * connecting it to a Qdrant server running locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "\n",
    "collection_name=\"chat_with_docs\" # used later to create a vector store\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Documents\n",
    "\n",
    "* set up a document loader that reads files from a specified directory and \n",
    "    * extracts their contents for use in our RAG pipeline.\n",
    "* allow us to retrieve text from PDF files, \n",
    "    * which we’ll later transform into embeddings and \n",
    "    * store in the Qdrant vector database created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('F:/cc_data/docs')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "data_dir = Path(r\"F:\\cc_data\\docs\")\n",
    "\n",
    "data_dir.absolute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* import the `SimpleDirectoryReader` class from the `llama_index` library. \n",
    "* This scans a directory, \n",
    "    * filters for specific file types, and \n",
    "    * loads document content into a format we can work with.\n",
    "* Next, we specify the directory path where additional documents are stored.\n",
    "* The loader is a `SimpleDirectoryReader` instance, \n",
    "    * which is configured to load specific types of files (`.pdf`) from the given directory, recursively.\n",
    "* So far, the loader object has only been instantiated. \n",
    "    * We haven't read anything yet. \n",
    "    * Thus, the `load_data()` method is used to read the PDF file’s content and \n",
    "    * return it in a structured format, storing it in docs list. \n",
    "* Each entry in docs represents the text content of the PDF document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# input_dir_path = './docs'\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "            input_dir = data_dir,\n",
    "            required_exts=[\".pdf\"],\n",
    "            recursive=True\n",
    "        )\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 32 pages in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs), len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to index data\n",
    "\n",
    "* Before embedding the above data and indexing in the vector database, \n",
    "    * we need to write a function that can be invoked to do this.\n",
    "\n",
    "* In this step, we'll define a function to create an index for our document embeddings, \n",
    "    * which will be stored in the Qdrant vector database.\n",
    "\n",
    "* This index allows us to organize and search through the document embeddings efficiently, \n",
    "    * forming the backbone of our RAG app.\n",
    "\n",
    "* break down how this function initializes the vector store, \n",
    "    * configures the storage context, and \n",
    "    * creates an index from the loaded documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above function will take the documents as input \n",
    "    * (`docs` which we created above using `SimpleDirectoryReader`).\n",
    "* We initialize a `QdrantVectorStore` object by \n",
    "    * passing the previously created Qdrant client in step 3 and \n",
    "    * a name for the collection.\n",
    "* Next, we configure storage settings by specifying the above `vector_store` as the storage backend.\n",
    "* Finally, we create an index by embedding each document in documents and \n",
    "    * storing it in the Qdrant vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
    "\n",
    "def create_index(documents):\n",
    "\n",
    "    vector_store = QdrantVectorStore(client=client,\n",
    "                                     collection_name=collection_name)\n",
    "    \n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    index = VectorStoreIndex.from_documents(documents,\n",
    "                                            storage_context=storage_context)\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the embedding model and index data\n",
    "\n",
    "* Now that we have defined the above function, we can actually index our data.\n",
    "\n",
    "* In this step, we are setting up an `embedding model` from `Hugging Face` to \n",
    "    * convert our documents into vector embeddings, \n",
    "    * which we’ll then store in Qdrant using our index function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The HuggingFaceEmbedding class lets us use Hugging Face models to generate embeddings for text data. \n",
    "    * In this case, we use \"BAAI/bge-large-en-v1.5\" model by the Beijing Academy of Artificial Intelligence (BAAI).\n",
    "* Next, we configure `embed_model` as the default embedding model in `Settings`. \n",
    "    * This setting ensures that the same model is used throughout our RAG pipeline to maintain consistency in embedding generation.\n",
    "* Finally, we invoke the `create_index` function we defined earlier, \n",
    "    * passing in docs (the list of loaded documents). \n",
    "    \n",
    "    * As discussed above, this function converts each document --> into an embedding using embed_model and --> stores the embeddings in Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "                                   trust_remote_code=True)\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "index = create_index(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load the LLM\n",
    "* In this step, we configure an LLM to handle the response generation step in our RAG pipeline.\n",
    "* First, we import the Ollama class. \n",
    "    * Here, we’re using the \"llama3.2:1b\" model.\n",
    "* We are also specifying a `request_timeout` of 120 seconds for requests \n",
    "    * to the LLM to ensure that the system doesn’t get stuck if the model takes too long to respond.\n",
    "* Finally, we set the above LLM instance as the default language model in `Settings`, \n",
    "    * making it the primary model used in our RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:1b\", request_timeout=120.0)\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the prompt template\n",
    "\n",
    "* In this step, we create a prompt template that defines a consistent format \n",
    "    * to guide the LLM about the context it should look at while answering the query.\n",
    "\n",
    "* First, we import the `PromptTemplate` class, \n",
    "    * which lets us define reusable prompt formats for the LLM.\n",
    "* Next, we define the template as a string, \n",
    "    * which is then passed to the `PromptTemplate` class to initialize its object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"Context information is below:\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner,\n",
    "              incase you don't know the answer say 'I don't know!'\n",
    "            \n",
    "              Query: {query_str}\n",
    "        \n",
    "              Answer:\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking\n",
    "\n",
    "* After retrieval, the selected chunks might need further refinement \n",
    "    * to ensure the most relevant information is prioritized.\n",
    "\n",
    "* In this re-ranking step, a more sophisticated model (often a `cross-encoder`) \n",
    "    * evaluates the initial list of retrieved chunks \n",
    "    * alongside the `query` to assign a relevance score to each chunk.\n",
    "\n",
    "* This process rearranges the chunks so that the most relevant ones are prioritized for the response generation.\n",
    "\n",
    "* Here, we use a cross-encoder to re-rank the document chunks.\n",
    "* Also, we limit the output to the top 3 most relevant chunks based on the model’s scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", \n",
    "    top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the document\n",
    "* Finally, we utilize the index created above to set up a query engine, \n",
    "    * which will use our indexed documents and \n",
    "    * re-ranking model to process user queries.\n",
    "\n",
    "* The query engine integrates the \n",
    "    * retrieval, \n",
    "    * re-ranking, and \n",
    "    * prompt-based response generation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, we convert the previously created `index` into a `query engine`.\n",
    "* We specify that the `engine` should retrieve the top 10 most similar document `chunks` based on `vector` similarity to the `query`. \n",
    "* This forms the initial set of `chunks` for answering the `query`.\n",
    "* The `re-ranking` step is further added to this to refine the retrieved `chunks`. \n",
    "    * The rerank model will evaluate these `chunks` to select the most relevant ones for generating a response.\n",
    "* Next, we add the `prompt template` to our `query engine`.\n",
    "* Finally, we execute a `query` using the configured `query engine`. \n",
    "* In this case, \"What exactly is DSPy?\" is our sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10,\n",
    "                                     node_postprocessors=[rerank])\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What exactly is DSPy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "DSPy stands for Declarative Specifier for Programming Interfaces, which refers to a programming model used for abstracting and customizing natural language prompts (signatures) in deep learning pipelines."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for RAG evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Typically, when evaluating a RAG system, we do not have access to human-annotated datasets or reference answers since \n",
    "    * the downstream application can be HIGHLY specific due to the capabilities of LLMs.\n",
    "* So, we prefer self-contained or reference-free metrics that \n",
    "    * capture the “quality” of the generated response, \n",
    "    * which is precisely what matters in RAG applications.\n",
    "* [Ragas Paper](https://arxiv.org/pdf/2309.15217)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faithfulness\n",
    "\n",
    "* measures whether the generated response $a(q)$ “stays true” or is \"faithful\" to the retrieved context $c(q)$. \n",
    "* In other words, faithfulness checks that all claims or information presented in the answer \n",
    "    * can be directly inferred from the retrieved context.\n",
    "\n",
    "* **high** faithfulness score means the generated text uses \n",
    "    * ONLY the information provided in the retrieved documents \n",
    "    * without introducing irrelevant or fabricated details, i.e., hallucinations.\n",
    "\n",
    "* To measure faithfulness, \n",
    "    * we can use a multi-step approach involving another LLM to break down \n",
    "    * the generated answer into distinct statements, \n",
    "        * each representing a focused assertion.\n",
    "    * goal of this breakdown is to simplify longer, \n",
    "        * complex sentences into smaller, verifiable statements.\n",
    "* Faihfulness score:\n",
    "    * $S$: Total number of statrements.\n",
    "    * $V$: Number of supported statements.\n",
    "    * Faithfulness Score: \n",
    "        $$F = \\frac{V}{S}$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Relevance\n",
    "\n",
    "* Answer Relevance measures whether the generated answer $a(q)$\n",
    "    * directly addresses the user’s query in a meaningful and complete way.\n",
    "* This metric focuses on the relevance of the response rather than its factual accuracy. What??\n",
    "* It discourages:\n",
    "    * technically be correct but are either \n",
    "        * too broad, \n",
    "        * partially off-topic, or \n",
    "        * contain unnecessary information.\n",
    "* How different from `Faithfulness`?\n",
    "    * Faithfulness evaluates the answer wrt question. \n",
    "    * Answer Relevance evaluates the question with proxy questions.\n",
    "\n",
    "* Steps\n",
    "    * Generate proxy questions\n",
    "        * For each $a(q)$ prmpt LLM to generate alternative questions.\n",
    "    * Calculate similarity scores:\n",
    "        * $$AR = \\frac{1}{n} \\sum_{i=1}^n \\text{similarity}(q, q_i)$$\n",
    "        * where $n$ is number of generated questions.\n",
    "        * $simalirity(q, q_i)$: cosine similarity between embedding of $q$ and each $q_i$.\n",
    "* A high AR score indicates that the generated answer is well-aligned with the original question, \n",
    "    * as it can match a variety of questions that reflect the same intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Relevance\n",
    "\n",
    "* \"How relevant is my context?\" \n",
    "    * Did we not check this in `Faithfulness`?\n",
    "* context relevance measures how well the retrieved context $c(q)$ is to answer the specific question $q$. \n",
    "* Thus, it should discourage cases where \n",
    "    * irrelevant details are included in the context that can mislead the LLM during the generation stage.\n",
    "* $$CR = \\frac{Number-of-relevant-sentences}{Number-of-Sentences-in-c(q)}$$\n",
    "* A high CR score indicates that\n",
    "    * majority of the sentences in the retrieved context are directly relevant to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Correctness\n",
    "\n",
    "* Answer correctness considers two key aspects—\n",
    "    * the semantic similarity between the generated answer and \n",
    "    * the ground truth, as well as factual similarity.\n",
    "* This requires 2 models:\n",
    "    * **critic LLM** \n",
    "        * determines factual correctness by measuring the similarity between \n",
    "            * the generated answer and \n",
    "            * the ground truth answer using a critic llm.\n",
    "    * **embedding model**\n",
    "        * computes the embeddings for the generated answer and the ground truth,\n",
    "        * and then measures the cosine similarity. \n",
    "        * This helps in determining the semantic similarity.\n",
    "\n",
    "```\n",
    "Given a ground truth and an answer, analyze each statement in the answer and classify them in one of the following categories:\n",
    "\n",
    "- TP (true positive): statements that are present in both the answer and the ground truth,\n",
    "- FP (false positive): statements present in the answer but not found in the ground truth,\n",
    "- FN (false negative): relevant statements found in the ground truth but omitted in the answer.\n",
    "\n",
    "Factual correctness score = TP / (TP + 0.5 * (FP + FN))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
