{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the notebook, make sure the following dependencies are installed:\n",
    "\n",
    "1. LlamaIndex\n",
    "    * Open source framework to connect your LLMs with external data sources.\n",
    "        * pdfs, websites, APIs.\n",
    "        * supports popular LLMs & vector databases.\n",
    "    * simplifies indexing & querying data.\n",
    "\n",
    "2. Qdrant\n",
    "    * open source vector database optimized for querying high dimensional vectors\n",
    "    * Use docker to self host a Qdrant vector database\n",
    "    * provides fast similarity and filtering search\n",
    "    * Install Docker Engine (if not already)\n",
    "    * Start the docker container for qdrant\n",
    "    * `docker run -p 6333:6333 -p 6334:6334 \\\n",
    "    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n",
    "    qdrant/qdrant`\n",
    "        * `docker run`: Initiates a new container.\n",
    "        * `-p 6333:6333 -p 6334:6334`: Maps the host machine's ports 6333 and 6334 to the same ports on the container.\n",
    "        * `-v $(pwd)/qdrant_storage:/qdrant/storage:z`:\n",
    "            * `$(pwd)/qdrant_storage` is a directory on the host (the current working directory with a subdirectory named qdrant_storage).\n",
    "            * `/qdrant/storage` is the corresponding directory inside the container where Qdrant will store its data.\n",
    "            * The `:z` suffix sets the SELinux context for the volume (on SELinux-enabled systems), allowing the container to access the mounted volume securely.\n",
    "        * `qdrant/qdrant`: Specifies the Docker image to use, which is `qdrant/qdrant` from Docker Hub set up by the Qdrant team.\n",
    "\n",
    "3. Ollama\n",
    "    * provides a platform to run LLMs locally, giivng control over the data and model usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To handle asynchronous operations within a Jupyter Notebook environment, \n",
    "# which will allow us to make asynchronous calls smoothly in our RAG \n",
    "\n",
    "# nest_asyncio is a library that allows you to use asyncio in environments \n",
    "# that don't natively support it, like Jupyter Notebooks.\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "# This line modifies the event loop policy to allow nested use of asyncio.run(). Essentially, \n",
    "# it makes sure that the event loop can handle multiple asynchronous calls without running into issues.\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Qdrant vector database\n",
    "\n",
    " set up a connection to Qdrant (our local vector database), where we will store and retrieve vector embeddings in this RAG\n",
    "\n",
    "* Collections in Qdrant are like tables in databases, \n",
    "    * where each collection can hold a set of vectors. \n",
    "* Here, \"chat_with_docs\" is intended to store document embeddings \n",
    "    * to support query-based information retrieval.\n",
    "* client = qdrant_client.QdrantClient(...) initializes a QdrantClient instance, \n",
    "    * connecting it to a Qdrant server running locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "\n",
    "collection_name=\"chat_with_docs\" # used later to create a vector store\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Documents\n",
    "\n",
    "* set up a document loader that reads files from a specified directory and \n",
    "    * extracts their contents for use in our RAG pipeline.\n",
    "* allow us to retrieve text from PDF files, \n",
    "    * which we’ll later transform into embeddings and \n",
    "    * store in the Qdrant vector database created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('F:/cc_data/docs')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "data_dir = Path(r\"F:\\cc_data\\docs\")\n",
    "\n",
    "data_dir.absolute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* import the `SimpleDirectoryReader` class from the `llama_index` library. \n",
    "* This scans a directory, \n",
    "    * filters for specific file types, and \n",
    "    * loads document content into a format we can work with.\n",
    "* Next, we specify the directory path where additional documents are stored.\n",
    "* The loader is a `SimpleDirectoryReader` instance, \n",
    "    * which is configured to load specific types of files (`.pdf`) from the given directory, recursively.\n",
    "* So far, the loader object has only been instantiated. \n",
    "    * We haven't read anything yet. \n",
    "    * Thus, the `load_data()` method is used to read the PDF file’s content and \n",
    "    * return it in a structured format, storing it in docs list. \n",
    "* Each entry in docs represents the text content of the PDF document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# input_dir_path = './docs'\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "            input_dir = data_dir,\n",
    "            required_exts=[\".pdf\"],\n",
    "            recursive=True\n",
    "        )\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 32 pages in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs), len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to index data\n",
    "\n",
    "* Before embedding the above data and indexing in the vector database, \n",
    "    * we need to write a function that can be invoked to do this.\n",
    "\n",
    "* In this step, we'll define a function to create an index for our document embeddings, \n",
    "    * which will be stored in the Qdrant vector database.\n",
    "\n",
    "* This index allows us to organize and search through the document embeddings efficiently, \n",
    "    * forming the backbone of our RAG app.\n",
    "\n",
    "* break down how this function initializes the vector store, \n",
    "    * configures the storage context, and \n",
    "    * creates an index from the loaded documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above function will take the documents as input \n",
    "    * (`docs` which we created above using `SimpleDirectoryReader`).\n",
    "* We initialize a `QdrantVectorStore` object by \n",
    "    * passing the previously created Qdrant client in step 3 and \n",
    "    * a name for the collection.\n",
    "* Next, we configure storage settings by specifying the above `vector_store` as the storage backend.\n",
    "* Finally, we create an index by embedding each document in documents and \n",
    "    * storing it in the Qdrant vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
    "\n",
    "def create_index(documents):\n",
    "\n",
    "    vector_store = QdrantVectorStore(client=client,\n",
    "                                     collection_name=collection_name)\n",
    "    \n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    index = VectorStoreIndex.from_documents(documents,\n",
    "                                            storage_context=storage_context)\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the embedding model and index data\n",
    "\n",
    "* Now that we have defined the above function, we can actually index our data.\n",
    "\n",
    "* In this step, we are setting up an `embedding model` from `Hugging Face` to \n",
    "    * convert our documents into vector embeddings, \n",
    "    * which we’ll then store in Qdrant using our index function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The HuggingFaceEmbedding class lets us use Hugging Face models to generate embeddings for text data. \n",
    "    * In this case, we use \"BAAI/bge-large-en-v1.5\" model by the Beijing Academy of Artificial Intelligence (BAAI).\n",
    "* Next, we configure `embed_model` as the default embedding model in `Settings`. \n",
    "    * This setting ensures that the same model is used throughout our RAG pipeline to maintain consistency in embedding generation.\n",
    "* Finally, we invoke the `create_index` function we defined earlier, \n",
    "    * passing in docs (the list of loaded documents). \n",
    "    \n",
    "    * As discussed above, this function converts each document --> into an embedding using embed_model and --> stores the embeddings in Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseHandlingException",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\httpx\\_transports\\default.py:72\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\httpx\\_transports\\default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\httpcore\\_sync\\connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\httpcore\\_sync\\connection.py:78\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\httpcore\\_sync\\connection.py:124\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m--> 124\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\httpcore\\_backends\\sync.py:207\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[1;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[0;32m    202\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    203\u001b[0m     socket\u001b[38;5;241m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[0;32m    205\u001b[0m }\n\u001b[1;32m--> 207\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[1;34m(map)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mConnectError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\qdrant_client\\http\\api_client.py:106\u001b[0m, in \u001b[0;36mApiClient.send_inner\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 106\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\httpx\\_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\httpx\\_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\httpx\\_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    989\u001b[0m     hook(request)\n\u001b[1;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\httpx\\_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\httpx\\_transports\\default.py:235\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    234\u001b[0m )\n\u001b[1;32m--> 235\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\httpx\\_transports\\default.py:89\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     88\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mConnectError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResponseHandlingException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m HuggingFaceEmbedding(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBAAI/bge-large-en-v1.5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m                                    trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m Settings\u001b[38;5;241m.\u001b[39membed_model \u001b[38;5;241m=\u001b[39m embed_model\n\u001b[1;32m----> 9\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m, in \u001b[0;36mcreate_index\u001b[1;34m(documents)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_index\u001b[39m(documents):\n\u001b[1;32m----> 6\u001b[0m     vector_store \u001b[38;5;241m=\u001b[39m \u001b[43mQdrantVectorStore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     storage_context \u001b[38;5;241m=\u001b[39m StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults(vector_store\u001b[38;5;241m=\u001b[39mvector_store)\n\u001b[0;32m     11\u001b[0m     index \u001b[38;5;241m=\u001b[39m VectorStoreIndex\u001b[38;5;241m.\u001b[39mfrom_documents(documents,\n\u001b[0;32m     12\u001b[0m                                             storage_context\u001b[38;5;241m=\u001b[39mstorage_context)\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\llama_index\\vector_stores\\qdrant\\base.py:200\u001b[0m, in \u001b[0;36mQdrantVectorStore.__init__\u001b[1;34m(self, collection_name, client, aclient, url, api_key, batch_size, parallel, max_retries, client_kwargs, dense_config, sparse_config, quantization_config, enable_hybrid, fastembed_sparse_model, sparse_doc_fn, sparse_query_fn, hybrid_fusion_fn, index_doc_id, text_key, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aclient \u001b[38;5;241m=\u001b[39m aclient\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collection_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m#  need to do lazy init for async clients\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\llama_index\\vector_stores\\qdrant\\base.py:711\u001b[0m, in \u001b[0;36mQdrantVectorStore._collection_exists\u001b[1;34m(self, collection_name)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_collection_exists\u001b[39m(\u001b[38;5;28mself\u001b[39m, collection_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    710\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check if a collection exists.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 711\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\qdrant_client\\qdrant_client.py:2097\u001b[0m, in \u001b[0;36mQdrantClient.collection_exists\u001b[1;34m(self, collection_name, **kwargs)\u001b[0m\n\u001b[0;32m   2087\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check whether collection already exists\u001b[39;00m\n\u001b[0;32m   2088\u001b[0m \n\u001b[0;32m   2089\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;124;03m    True if collection exists, False if not\u001b[39;00m\n\u001b[0;32m   2094\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2095\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2097\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\qdrant_client\\qdrant_remote.py:2623\u001b[0m, in \u001b[0;36mQdrantRemote.collection_exists\u001b[1;34m(self, collection_name, **kwargs)\u001b[0m\n\u001b[0;32m   2617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefer_grpc:\n\u001b[0;32m   2618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrpc_collections\u001b[38;5;241m.\u001b[39mCollectionExists(\n\u001b[0;32m   2619\u001b[0m         grpc\u001b[38;5;241m.\u001b[39mCollectionExistsRequest(collection_name\u001b[38;5;241m=\u001b[39mcollection_name),\n\u001b[0;32m   2620\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[0;32m   2621\u001b[0m     )\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mexists\n\u001b[1;32m-> 2623\u001b[0m result: Optional[models\u001b[38;5;241m.\u001b[39mCollectionExistence] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollections_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection_exists\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\n\u001b[0;32m   2625\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresult\n\u001b[0;32m   2626\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection exists returned None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mexists\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\qdrant_client\\http\\api\\collections_api.py:1157\u001b[0m, in \u001b[0;36mSyncCollectionsApi.collection_exists\u001b[1;34m(self, collection_name)\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollection_exists\u001b[39m(\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1152\u001b[0m     collection_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1153\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m m\u001b[38;5;241m.\u001b[39mInlineResponse2007:\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;124;03m    Returns \\\"true\\\" if the given collection name exists, and \\\"false\\\" otherwise\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_for_collection_exists\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\qdrant_client\\http\\api\\collections_api.py:87\u001b[0m, in \u001b[0;36m_CollectionsApi._build_for_collection_exists\u001b[1;34m(self, collection_name)\u001b[0m\n\u001b[0;32m     82\u001b[0m path_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollection_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(collection_name),\n\u001b[0;32m     84\u001b[0m }\n\u001b[0;32m     86\u001b[0m headers \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInlineResponse2007\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/collections/\u001b[39;49m\u001b[38;5;132;43;01m{collection_name}\u001b[39;49;00m\u001b[38;5;124;43m/exists\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\qdrant_client\\http\\api_client.py:79\u001b[0m, in \u001b[0;36mApiClient.request\u001b[1;34m(self, type_, method, url, path_params, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     78\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mbuild_request(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\qdrant_client\\http\\api_client.py:96\u001b[0m, in \u001b[0;36mApiClient.send\u001b[1;34m(self, request, type_)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: Request, type_: Type[T]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m---> 96\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmiddleware\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_inner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m201\u001b[39m, \u001b[38;5;241m202\u001b[39m]:\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\qdrant_client\\http\\api_client.py:205\u001b[0m, in \u001b[0;36mBaseMiddleware.__call__\u001b[1;34m(self, request, call_next)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: Request, call_next: Send) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\qdrant_client\\http\\api_client.py:108\u001b[0m, in \u001b[0;36mApiClient.send_inner\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    106\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResponseHandlingException(e)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[1;31mResponseHandlingException\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "                                   trust_remote_code=True)\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "index = create_index(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load the LLM\n",
    "* In this step, we configure an LLM to handle the response generation step in our RAG pipeline.\n",
    "* First, we import the Ollama class. \n",
    "    * Here, we’re using the \"llama3.2:1b\" model.\n",
    "* We are also specifying a `request_timeout` of 120 seconds for requests \n",
    "    * to the LLM to ensure that the system doesn’t get stuck if the model takes too long to respond.\n",
    "* Finally, we set the above LLM instance as the default language model in `Settings`, \n",
    "    * making it the primary model used in our RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:1b\", request_timeout=120.0)\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the prompt template\n",
    "\n",
    "* In this step, we create a prompt template that defines a consistent format \n",
    "    * to guide the LLM about the context it should look at while answering the query.\n",
    "\n",
    "* First, we import the `PromptTemplate` class, \n",
    "    * which lets us define reusable prompt formats for the LLM.\n",
    "* Next, we define the template as a string, \n",
    "    * which is then passed to the `PromptTemplate` class to initialize its object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"Context information is below:\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner,\n",
    "              incase you don't know the answer say 'I don't know!'\n",
    "            \n",
    "              Query: {query_str}\n",
    "        \n",
    "              Answer:\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking\n",
    "\n",
    "* After retrieval, the selected chunks might need further refinement \n",
    "    * to ensure the most relevant information is prioritized.\n",
    "\n",
    "* In this re-ranking step, a more sophisticated model (often a `cross-encoder`) \n",
    "    * evaluates the initial list of retrieved chunks \n",
    "    * alongside the `query` to assign a relevance score to each chunk.\n",
    "\n",
    "* This process rearranges the chunks so that the most relevant ones are prioritized for the response generation.\n",
    "\n",
    "* Here, we use a cross-encoder to re-rank the document chunks.\n",
    "* Also, we limit the output to the top 3 most relevant chunks based on the model’s scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", \n",
    "    top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the document\n",
    "* Finally, we utilize the index created above to set up a query engine, \n",
    "    * which will use our indexed documents and \n",
    "    * re-ranking model to process user queries.\n",
    "\n",
    "* The query engine integrates the \n",
    "    * retrieval, \n",
    "    * re-ranking, and \n",
    "    * prompt-based response generation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, we convert the previously created `index` into a `query engine`.\n",
    "* We specify that the `engine` should retrieve the top 10 most similar document `chunks` based on `vector` similarity to the `query`. \n",
    "* This forms the initial set of `chunks` for answering the `query`.\n",
    "* The `re-ranking` step is further added to this to refine the retrieved `chunks`. \n",
    "    * The rerank model will evaluate these `chunks` to select the most relevant ones for generating a response.\n",
    "* Next, we add the `prompt template` to our `query engine`.\n",
    "* Finally, we execute a `query` using the configured `query engine`. \n",
    "* In this case, \"What exactly is DSPy?\" is our sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10,\n",
    "                                     node_postprocessors=[rerank])\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What exactly is DSPy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "DSPy stands for Declarative Specifier for Programming Interfaces, which refers to a programming model used for abstracting and customizing natural language prompts (signatures) in deep learning pipelines."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 2 - Metrics for RAG evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Typically, when evaluating a RAG system, we do not have access to human-annotated datasets or reference answers since \n",
    "    * the downstream application can be HIGHLY specific due to the capabilities of LLMs.\n",
    "* So, we prefer self-contained or reference-free metrics that \n",
    "    * capture the “quality” of the generated response, \n",
    "    * which is precisely what matters in RAG applications.\n",
    "* [Ragas Paper](https://arxiv.org/pdf/2309.15217)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import qdrant_client\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:32<00:00,  6.40s/it]\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3.2:1b\", request_timeout=120.0)\n",
    "\n",
    "embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "rerank = SentenceTransformerRerank(model=\"BAAI/bge-reranker-base\", top_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = embed_model\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_path = data_dir.absolute() / 'paul_graham'\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "            input_dir = input_dir_path,\n",
    "            required_exts=[\".txt\"],\n",
    "            recursive=True\n",
    "        )\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qdrant_client.QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client,\n",
    "                                 collection_name=\"document_chat\")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(docs,\n",
    "                                        storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=4,\n",
    "                                     node_postprocessors=[rerank])\n",
    "\n",
    "template = \"\"\"Context information is below.\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner. Incase \n",
    "              you don't know the answer say 'I don't know!'.\n",
    "              \n",
    "              Query: {query_str}\n",
    "              \n",
    "              Answer:\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(template)\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The structure of funding startups in batches, which involves funding a group of startups at once, twice a year, contributed significantly to the success and growth of the Y Combinator (YC) program. Here's how:\n",
       "\n",
       "1. **Convenience for founders**: Funding startups in batches allowed YC to efficiently support multiple startups simultaneously, reducing the isolation that often comes with working on individual projects. This enabled founders to focus on their own projects while still benefiting from the expertise and resources provided by YC.\n",
       "2. **Improved mentorship**: The batch model facilitated a more intense and focused approach to helping startups. Experts could provide guidance and support without having to dedicate as much time to each individual project, leading to better outcomes for the startups.\n",
       "3. **Reduced administrative burden**: By not requiring founders to spend their time managing multiple investments simultaneously, YC reduced the administrative burden associated with running a fund. This allowed the team to focus on other aspects of the program.\n",
       "4. **Increased networking opportunities**: The batch model provided an excellent opportunity for founders to connect and learn from one another. This helped create a tight-knit community, where alumni became valuable resources and supporters.\n",
       "5. **Accelerated learning**: YC's experience in funding startups in batches allowed them to develop valuable lessons on how to effectively invest in early-stage companies. These experiences enabled the YC team to refine their investment approach over time.\n",
       "\n",
       "The success of YC can be attributed to the ability to scale startup funding by doing it efficiently and effectively, which was made possible by the batch model."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"\"\"How did the structure of funding startups \n",
    "                                 in batches contribute to the success and \n",
    "                                 growth of the Y Combinator program and the\n",
    "                                 startups involved?\"\"\")\n",
    "                                 \n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = DirectoryLoader(data_dir.absolute() / \"paul_graham\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "\n",
    "documents = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lc': 1,\n",
       " 'type': 'constructor',\n",
       " 'id': ['langchain', 'schema', 'document', 'Document'],\n",
       " 'kwargs': {'page_content': 'How to Do Great Work\\n\\nJuly 2023\\n\\nIf you collected lists of techniques for doing great work in a lot of different fields, what would the intersection look like? I decided to find out by making it.\\n\\nPartly my goal was to create a guide that could be used by someone working in any field. But I was also curious about the shape of the intersection. And one thing this exercise shows is that it does have a definite shape; it\\'s not just a point labelled \"work hard.\"\\n\\nThe following recipe assumes you\\'re very ambitious.\\n\\nThe first step is to decide what to work on. The work you choose needs to have three qualities: it has to be something you have a natural aptitude for, that you have a deep interest in, and that offers scope to do great work.\\n\\nIn practice you don\\'t have to worry much about the third criterion. Ambitious people are if anything already too conservative about it. So all you need to do is find something you have an aptitude for and great interest in. [1]',\n",
       "  'metadata': {'source': 'F:\\\\cc_data\\\\docs\\\\paul_graham\\\\how_to_do_great_things.txt'},\n",
       "  'type': 'Document'}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "generator_llm = Ollama(model=\"phi3:3.8b\")\n",
    "critic_llm = Ollama(model=\"llama3.2:1b\")\n",
    "\n",
    "ollama_emb = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LangSmithParams' from 'langchain_core.language_models.chat_models' (c:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtestset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TestsetGenerator\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtestset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevolutions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m simple, reasoning, multi_context\n\u001b[0;32m      4\u001b[0m generator \u001b[38;5;241m=\u001b[39m TestsetGenerator\u001b[38;5;241m.\u001b[39mfrom_langchain(\n\u001b[0;32m      5\u001b[0m     generator_llm\u001b[38;5;241m=\u001b[39mgenerator_llm,\n\u001b[0;32m      6\u001b[0m     critic_llm\u001b[38;5;241m=\u001b[39mcritic_llm,\n\u001b[0;32m      7\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39mollama_emb\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\ragas\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madaptation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adapt\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunConfig\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\ragas\\adaptation.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLanguageModel\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m llm_factory\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseRagasLLM, LangchainLLMWrapper\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MetricWithLLM\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\ragas\\llms\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseRagasLLM, LangchainLLMWrapper, llm_factory\n\u001b[0;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseRagasLLM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLangchainLLMWrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_factory\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\ragas\\llms\\base.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLanguageModel\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMResult\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI, ChatOpenAI\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, OpenAI\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOpenAI\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\langchain_openai\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI, ChatOpenAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIEmbeddings, OpenAIEmbeddings\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, OpenAI\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\langchain_openai\\chat_models\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzureChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\langchain_openai\\chat_models\\azure.py:21\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     Any,\n\u001b[0;32m      9\u001b[0m     Awaitable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     Union,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LangSmithParams\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseMessage\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatResult\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LangSmithParams' from 'langchain_core.language_models.chat_models' (c:\\Users\\akhan147\\Anaconda3\\envs\\rag\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py)"
     ]
    }
   ],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm=generator_llm,\n",
    "    critic_llm=critic_llm,\n",
    "    embeddings=ollama_emb\n",
    ")\n",
    "\n",
    "distribution = {simple: 0.5, reasoning: 0.25, multi_context: 0.25}\n",
    "testset = generator.generate_with_langchain_docs(documents,\n",
    "                                                 test_size=10,\n",
    "                                                 distributions=distribution,\n",
    "                                                 raise_exceptions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv(input_dir_path / 'test_data_paul_graham.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query_engine, question):\n",
    "    response = query_engine.query(question)\n",
    "    return {\n",
    "        \"answer\": response.response,\n",
    "        \"contexts\": [c.node.get_content() for c in response.source_nodes],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 6/47 [01:08<07:26, 10.90s/it]"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "test_questions = test_df[\"question\"].values\n",
    "\n",
    "responses = [generate_response(query_engine, q) for q in tqdm(test_questions)]\n",
    "\n",
    "dataset_dict = {\n",
    "    \"question\": test_questions,\n",
    "    \"answer\": [response[\"answer\"] for response in responses],\n",
    "    \"contexts\": [response[\"contexts\"] for response in responses],\n",
    "    \"ground_truth\": test_df[\"ground_truth\"].values.tolist(),\n",
    "}\n",
    "\n",
    "ragas_eval_dataset = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [faithfulness, answer_correctness,\n",
    "           context_recall, context_precision]\n",
    "\n",
    "critic_llm = Ollama(model=\"llama3.2:1b\")\n",
    "\n",
    "ollama_emb = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "evaluation_result = evaluate(\n",
    "    llm=critic_llm,\n",
    "    embeddings=ollama_emb,\n",
    "    dataset=ragas_eval_dataset,\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scores_df = pd.DataFrame(evaluation_result.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faithfulness\n",
    "\n",
    "* measures whether the generated response $a(q)$ “stays true” or is \"faithful\" to the retrieved context $c(q)$. \n",
    "* In other words, faithfulness checks that all claims or information presented in the answer \n",
    "    * can be directly inferred from the retrieved context.\n",
    "\n",
    "* **high** faithfulness score means the generated text uses \n",
    "    * ONLY the information provided in the retrieved documents \n",
    "    * without introducing irrelevant or fabricated details, i.e., hallucinations.\n",
    "\n",
    "* To measure faithfulness, \n",
    "    * we can use a multi-step approach involving another LLM to break down \n",
    "    * the generated answer into distinct statements, \n",
    "        * each representing a focused assertion.\n",
    "    * goal of this breakdown is to simplify longer, \n",
    "        * complex sentences into smaller, verifiable statements.\n",
    "* Faihfulness score:\n",
    "    * $S$: Total number of statrements.\n",
    "    * $V$: Number of supported statements.\n",
    "    * Faithfulness Score: \n",
    "        $$F = \\frac{V}{S}$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Relevance\n",
    "\n",
    "* Answer Relevance measures whether the generated answer $a(q)$\n",
    "    * directly addresses the user’s query in a meaningful and complete way.\n",
    "* This metric focuses on the relevance of the response rather than its factual accuracy. What??\n",
    "* It discourages:\n",
    "    * technically be correct but are either \n",
    "        * too broad, \n",
    "        * partially off-topic, or \n",
    "        * contain unnecessary information.\n",
    "* How different from `Faithfulness`?\n",
    "    * Faithfulness evaluates the answer wrt question. \n",
    "    * Answer Relevance evaluates the question with proxy questions.\n",
    "\n",
    "* Steps\n",
    "    * Generate proxy questions\n",
    "        * For each $a(q)$ prmpt LLM to generate alternative questions.\n",
    "    * Calculate similarity scores:\n",
    "        * $$AR = \\frac{1}{n} \\sum_{i=1}^n \\text{similarity}(q, q_i)$$\n",
    "        * where $n$ is number of generated questions.\n",
    "        * $simalirity(q, q_i)$: cosine similarity between embedding of $q$ and each $q_i$.\n",
    "* A high AR score indicates that the generated answer is well-aligned with the original question, \n",
    "    * as it can match a variety of questions that reflect the same intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Relevance\n",
    "\n",
    "* \"How relevant is my context?\" \n",
    "    * Did we not check this in `Faithfulness`?\n",
    "* context relevance measures how well the retrieved context $c(q)$ is to answer the specific question $q$. \n",
    "* Thus, it should discourage cases where \n",
    "    * irrelevant details are included in the context that can mislead the LLM during the generation stage.\n",
    "* $$CR = \\frac{Number-of-relevant-sentences}{Number-of-Sentences-in-c(q)}$$\n",
    "* A high CR score indicates that\n",
    "    * majority of the sentences in the retrieved context are directly relevant to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Correctness\n",
    "\n",
    "* Answer correctness considers two key aspects—\n",
    "    * the semantic similarity between the generated answer and \n",
    "    * the ground truth, as well as factual similarity.\n",
    "* This requires 2 models:\n",
    "    * **critic LLM** \n",
    "        * determines factual correctness by measuring the similarity between \n",
    "            * the generated answer and \n",
    "            * the ground truth answer using a critic llm.\n",
    "    * **embedding model**\n",
    "        * computes the embeddings for the generated answer and the ground truth,\n",
    "        * and then measures the cosine similarity. \n",
    "        * This helps in determining the semantic similarity.\n",
    "\n",
    "```\n",
    "Given a ground truth and an answer, analyze each statement in the answer and classify them in one of the following categories:\n",
    "\n",
    "- TP (true positive): statements that are present in both the answer and the ground truth,\n",
    "- FP (false positive): statements present in the answer but not found in the ground truth,\n",
    "- FN (false negative): relevant statements found in the ground truth but omitted in the answer.\n",
    "\n",
    "Factual correctness score = TP / (TP + 0.5 * (FP + FN))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 3 - Optimize RAG systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
